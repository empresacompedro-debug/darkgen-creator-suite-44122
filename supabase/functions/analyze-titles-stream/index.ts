import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import { createClient } from "https://esm.sh/@supabase/supabase-js@2";
import { buildGeminiOrVertexRequest } from '../_shared/vertex-helpers.ts';

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
};

serve(async (req) => {
  if (req.method === 'OPTIONS') {
    return new Response(null, { headers: corsHeaders });
  }

  try {
    const { rawData, aiModel = 'gemini-2.5-flash' } = await req.json();

    console.log('üì® [Analyze Titles Stream] Request received');
    console.log('üéØ [Analyze Titles Stream] Model:', aiModel);
    console.log('üìä [Analyze Titles Stream] Raw data length:', rawData?.length);

    if (!rawData || rawData.trim().length === 0) {
      throw new Error('Dados vazios. Por favor, cole os dados do YouTube.');
    }

    // Determinar provider baseado no prefixo do modelo
    let provider: 'gemini' | 'vertex-ai' | 'claude' | 'openai';
    let model = aiModel;

    if (aiModel.startsWith('gemini-')) {
      provider = 'gemini';
    } else if (aiModel.startsWith('vertex-')) {
      provider = 'vertex-ai';
      model = aiModel.replace('vertex-', ''); // Remove prefix
    } else if (aiModel.startsWith('claude')) {
      provider = 'claude';
    } else if (aiModel.startsWith('gpt') || aiModel.startsWith('o1') || aiModel.startsWith('o3')) {
      provider = 'openai';
    } else {
      provider = 'gemini'; // Default
    }

    console.log(`üîÑ [Analyze Titles Stream] Provider: ${provider}, Model: ${model}`);

    // Build comprehensive markdown prompt
    const prompt = `CONTEXTO:
Voc√™ √© um especialista em an√°lise de performance de conte√∫do no YouTube, especializado em identificar padr√µes virais em t√≠tulos de v√≠deos de qualquer nicho/subnicho e microsubnicho.

TAREFA:
Analise os t√≠tulos fornecidos e crie uma resposta seguindo RIGOROSAMENTE este modelo:

# üèÜ **TEMA CAMPE√ÉO ABSOLUTO**
[Identificar o tema principal de maior sucesso combinando 3 elementos: CONTEXTO + CONFLITO + RESULTADO]

## **üîë TOP 10 PALAVRAS-CHAVE MAIS REPETIDAS**
1. **"[Palavra/Frase]"** - [N¬∫ vezes]x (m√©dia [X]K views)
2. **"[Palavra/Frase]"** - [N¬∫ vezes]x (m√©dia [X]K views)
[Continue at√© 10...]

## **üìä 5 SUBNICHOS CAMPE√ïES**
1. **[Nome do Subnicho]** - M√©dia [X]K views
2. **[Nome do Subnicho]** - M√©dia [X]K views
[Continue at√© 5...]

## **üéØ 10 MICRONICHOS CAMPE√ïES**
1. **"[Descri√ß√£o Espec√≠fica do Micronicho]"** - [X]K m√©dia
2. **"[Descri√ß√£o Espec√≠fica do Micronicho]"** - [X]K m√©dia
[Continue at√© 10...]

## **‚ú® 50 NOVOS T√çTULOS BASEADOS NOS 5 CAMPE√ïES**

### **BASEADOS NO CAMPE√ÉO 1 ([X]K views):**
**"[T√≠tulo original completo]"**
1. [Nova varia√ß√£o mantendo estrutura mas mudando detalhes]
2. [Nova varia√ß√£o mantendo estrutura mas mudando detalhes]
[Continue at√© 10...]

### **BASEADOS NO CAMPE√ÉO 2 ([X]K views):**
**"[T√≠tulo original completo]"**
11. [Nova varia√ß√£o mantendo estrutura mas mudando detalhes]
12. [Nova varia√ß√£o mantendo estrutura mas mudando detalhes]
[Continue at√© 20...]

[Repetir para Campe√µes 3, 4 e 5 at√© completar 50 t√≠tulos]

## üí° **8 ELEMENTOS-CHAVE PARA REPLICAR**
1. **[Elemento]** (sempre incluir exemplo)
2. **[Elemento]** (sempre incluir exemplo)
[Continue at√© 8...]

## üöÄ **MICRONICHOS PARA REPLICAR**

### **PRIORIDADE 1 (FAZER IMEDIATAMENTE):**
- [Micronicho 1 com descri√ß√£o]
- [Micronicho 2 com descri√ß√£o]
- [Micronicho 3 com descri√ß√£o]

### **PRIORIDADE 2 (ALTA PERFORMANCE):**
- [Micronicho 4 com descri√ß√£o]
- [Micronicho 5 com descri√ß√£o]
- [Micronicho 6 com descri√ß√£o]

### **PRIORIDADE 3 (BOA PERFORMANCE):**
- [Micronicho 7 com descri√ß√£o]
- [Micronicho 8 com descri√ß√£o]
- [Micronicho 9 com descri√ß√£o]

## ‚≠ê **10 T√çTULOS FINAIS COM MAIOR POTENCIAL**

**1. MICRONICHO: [Nome do Micronicho] [Potencial: XXK+ views]**
\`\`\`
[T√≠tulo completo de 15-20 palavras seguindo a f√≥rmula identificada]
\`\`\`

**2. MICRONICHO: [Nome do Micronicho] [Potencial: XXK+ views]**
\`\`\`
[T√≠tulo completo de 15-20 palavras seguindo a f√≥rmula identificada]
\`\`\`

[Repetir para 10 t√≠tulos]

=== DADOS DE ENTRADA ===
${rawData}

IMPORTANTE: 
- N√ÉO adicione se√ß√µes extras
- N√ÉO mude a ordem das se√ß√µes
- MANTENHA exatamente a formata√ß√£o mostrada
- USE os mesmos emojis indicados
- SEMPRE baseie as varia√ß√µes nos 5 campe√µes identificados
- Retorne APENAS o markdown formatado, sem explica√ß√µes adicionais`;

    // Criar streaming response
    const stream = new ReadableStream({
      async start(controller) {
        const encoder = new TextEncoder();

        try {
          // GEMINI GRATUITO - nunca consulta user_api_keys
          if (provider === 'gemini') {
            console.log('üì° [Gemini Free] Starting request...');
            
            const apiKey = Deno.env.get('GEMINI_API_KEY');
            if (!apiKey) {
              throw new Error('GEMINI_API_KEY n√£o configurada');
            }

            console.log('üîë [Gemini Free] Using global key');

            // Usar helper para construir request com stream
            const { url, headers, body } = await buildGeminiOrVertexRequest(
              { key: apiKey },
              model,
              prompt,
              true // stream = true
            );

            const response = await fetch(url, {
              method: 'POST',
              headers,
              body: JSON.stringify(body),
            });

            if (!response.ok) {
              const errorText = await response.text();
              console.error('‚ùå [Gemini Free] API error:', response.status, errorText);
              throw new Error(`Gemini API error: ${response.status} - ${errorText}`);
            }

            console.log('‚úÖ [Gemini Free] Response received, starting stream...');

            const reader = response.body!.getReader();
            const decoder = new TextDecoder();

            while (true) {
              const { done, value } = await reader.read();
              if (done) {
                console.log('‚úÖ [Gemini Free] Stream complete');
                break;
              }

              const text = decoder.decode(value, { stream: true });
              const lines = text.split('\n');

              for (const line of lines) {
                if (line.startsWith('data: ')) {
                  const jsonStr = line.slice(6).trim();
                  if (!jsonStr) continue;

                  try {
                    const data = JSON.parse(jsonStr);
                    const chunk = data.candidates?.[0]?.content?.parts?.[0]?.text;
                    if (chunk) {
                      controller.enqueue(encoder.encode(`data: ${JSON.stringify({ text: chunk })}\n\n`));
                    }
                  } catch (e) {
                    console.error('‚ùå [Gemini Free] Error parsing SSE:', e);
                  }
                }
              }
            }
          }
          // VERTEX AI - exige chave do usu√°rio, SEM FALLBACK
          else if (provider === 'vertex-ai') {
            console.log('üì° [Vertex AI] Starting request...');

            // Get Supabase client
            const supabaseUrl = Deno.env.get('SUPABASE_URL')!;
            const supabaseKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!;
            const supabase = createClient(supabaseUrl, supabaseKey);

            // Get user from auth header
            const authHeader = req.headers.get('Authorization');
            let userId: string | null = null;
            
            if (authHeader) {
              const token = authHeader.replace('Bearer ', '');
              const { data: { user }, error: authError } = await supabase.auth.getUser(token);
              if (!authError && user) {
                userId = user.id;
              }
            }

            if (!userId) {
              controller.enqueue(encoder.encode(`data: ${JSON.stringify({ error: 'Autentica√ß√£o necess√°ria para usar Vertex AI' })}\n\n`));
              controller.close();
              return;
            }

            console.log('üë§ [Vertex AI] User ID:', userId);

            // Buscar chave Vertex AI do usu√°rio
            const { data: keyData, error: keyError } = await supabase
              .from('user_api_keys')
              .select('api_key_encrypted, vertex_config')
              .eq('user_id', userId)
              .eq('api_provider', 'vertex-ai')
              .eq('is_active', true)
              .order('priority', { ascending: true })
              .limit(1)
              .single();

            if (keyError || !keyData) {
              console.error('‚ùå [Vertex AI] Nenhuma chave encontrada para o usu√°rio');
              controller.enqueue(encoder.encode(`data: ${JSON.stringify({ error: 'Configure sua chave Vertex AI nas configura√ß√µes' })}\n\n`));
              controller.close();
              return;
            }

            // Descriptografar chave
            const { data: decrypted, error: decErr } = await supabase.rpc('decrypt_api_key', {
              p_encrypted: keyData.api_key_encrypted,
              p_user_id: userId,
            });

            if (decErr || !decrypted) {
              console.error('‚ùå [Vertex AI] Erro ao descriptografar chave');
              controller.enqueue(encoder.encode(`data: ${JSON.stringify({ error: 'Erro ao descriptografar chave Vertex AI' })}\n\n`));
              controller.close();
              return;
            }

            console.log('üîë [Vertex AI] Key decrypted successfully');

            const keyInfo = {
              key: decrypted as string,
              provider: 'vertex-ai' as const,
              vertexConfig: keyData.vertex_config
            };

            // Construir requisi√ß√£o com stream
            const { url, headers, body } = await buildGeminiOrVertexRequest(
              keyInfo,
              model,
              prompt,
              true // stream = true
            );

            const response = await fetch(url, {
              method: 'POST',
              headers,
              body: JSON.stringify(body),
            });

            if (!response.ok) {
              const errorText = await response.text();
              console.error('‚ùå [Vertex AI] API error:', response.status, errorText);
              
              if (response.status === 429) {
                controller.enqueue(encoder.encode(`data: ${JSON.stringify({ error: 'Limite de taxa excedido. Aguarde alguns minutos.' })}\n\n`));
              } else if (response.status === 402) {
                controller.enqueue(encoder.encode(`data: ${JSON.stringify({ error: 'Cr√©ditos insuficientes no Vertex AI.' })}\n\n`));
              } else {
                controller.enqueue(encoder.encode(`data: ${JSON.stringify({ error: `Vertex AI error: ${response.status}` })}\n\n`));
              }
              controller.close();
              return;
            }

            console.log('‚úÖ [Vertex AI] Response received, starting stream...');

            const reader = response.body!.getReader();
            const decoder = new TextDecoder();

            while (true) {
              const { done, value } = await reader.read();
              if (done) {
                console.log('‚úÖ [Vertex AI] Stream complete');
                break;
              }

              const text = decoder.decode(value, { stream: true });
              const lines = text.split('\n');

              for (const line of lines) {
                if (line.startsWith('data: ')) {
                  const jsonStr = line.slice(6).trim();
                  if (!jsonStr) continue;

                  try {
                    const data = JSON.parse(jsonStr);
                    const chunk = data.candidates?.[0]?.content?.parts?.[0]?.text;
                    if (chunk) {
                      controller.enqueue(encoder.encode(`data: ${JSON.stringify({ text: chunk })}\n\n`));
                    }
                  } catch (e) {
                    console.error('‚ùå [Vertex AI] Error parsing SSE:', e);
                  }
                }
              }
            }
          }
          // CLAUDE - streaming nativo
          else if (provider === 'claude') {
            console.log('üì° [Claude] Starting streaming request...');
            
            const apiKey = Deno.env.get('ANTHROPIC_API_KEY');
            if (!apiKey) {
              throw new Error('ANTHROPIC_API_KEY n√£o configurada');
            }

            const response = await fetch('https://api.anthropic.com/v1/messages', {
              method: 'POST',
              headers: {
                'x-api-key': apiKey,
                'anthropic-version': '2023-06-01',
                'content-type': 'application/json',
              },
              body: JSON.stringify({
                model: model,
                max_tokens: 8192,
                stream: true,
                messages: [{ role: 'user', content: prompt }]
              })
            });

            if (!response.ok) {
              const errorText = await response.text();
              console.error('‚ùå [Claude] API error:', response.status, errorText);
              throw new Error(`Claude API error: ${response.status} - ${errorText}`);
            }
            
            console.log('‚úÖ [Claude] Response received, starting stream read...');

            const reader = response.body!.getReader();
            const decoder = new TextDecoder();
            
            while (true) {
              const { done, value } = await reader.read();
              if (done) {
                console.log('‚úÖ [Claude] Stream complete');
                break;
              }

              const text = decoder.decode(value, { stream: true });
              const lines = text.split('\n');

              for (const line of lines) {
                if (line.startsWith('data: ')) {
                  const jsonStr = line.slice(6).trim();
                  if (!jsonStr) continue;

                  try {
                    const data = JSON.parse(jsonStr);
                    if (data.type === 'content_block_delta' && data.delta?.text) {
                      controller.enqueue(encoder.encode(`data: ${JSON.stringify({ text: data.delta.text })}\n\n`));
                    }
                  } catch (e) {
                    console.error('‚ùå [Claude] Error parsing SSE:', e);
                  }
                }
              }
            }
          }
          // OPENAI - streaming nativo
          else if (provider === 'openai') {
            console.log('üì° [OpenAI] Starting streaming request...');
            
            const apiKey = Deno.env.get('OPENAI_API_KEY');
            if (!apiKey) {
              throw new Error('OPENAI_API_KEY n√£o configurada');
            }

            const response = await fetch('https://api.openai.com/v1/chat/completions', {
              method: 'POST',
              headers: {
                'Authorization': `Bearer ${apiKey}`,
                'Content-Type': 'application/json'
              },
              body: JSON.stringify({
                model: model,
                stream: true,
                messages: [{ role: 'user', content: prompt }]
              })
            });

            if (!response.ok) {
              const errorText = await response.text();
              console.error('‚ùå [OpenAI] API error:', response.status, errorText);
              throw new Error(`OpenAI API error: ${response.status} - ${errorText}`);
            }

            console.log('‚úÖ [OpenAI] Response received, starting stream read...');
            const reader = response.body!.getReader();
            const decoder = new TextDecoder();

            while (true) {
              const { done, value } = await reader.read();
              if (done) {
                console.log('‚úÖ [OpenAI] Stream complete');
                break;
              }

              const text = decoder.decode(value, { stream: true });
              const lines = text.split('\n').filter(l => l.trim());

              for (const line of lines) {
                if (line.startsWith('data: ')) {
                  const jsonStr = line.slice(6).trim();
                  if (jsonStr === '[DONE]') break;
                  
                  try {
                    const data = JSON.parse(jsonStr);
                    const chunk = data.choices?.[0]?.delta?.content;
                    if (chunk) {
                      controller.enqueue(encoder.encode(`data: ${JSON.stringify({ text: chunk })}\n\n`));
                    }
                  } catch (e) {
                    console.error('‚ùå [OpenAI] Error parsing SSE:', e);
                  }
                }
              }
            }
          }

          // Enviar sinal de conclus√£o
          controller.enqueue(encoder.encode('data: [DONE]\n\n'));
          controller.close();
          console.log('‚úÖ [Analyze Titles Stream] Completed successfully');

        } catch (error: any) {
          console.error('‚ùå [Analyze Titles Stream] Error:', error);
          controller.enqueue(encoder.encode(`data: ${JSON.stringify({ error: error.message })}\n\n`));
          controller.close();
        }
      }
    });

    return new Response(stream, {
      headers: {
        ...corsHeaders,
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
      }
    });

  } catch (error: any) {
    console.error('‚ùå [Analyze Titles Stream] Fatal error:', error);
    return new Response(JSON.stringify({ error: error.message }), {
      status: 500,
      headers: { ...corsHeaders, 'Content-Type': 'application/json' }
    });
  }
});
